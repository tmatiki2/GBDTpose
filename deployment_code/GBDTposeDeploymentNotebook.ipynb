{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "import rospy\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import torchvision.models as models\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rospy.init_node(\"gbdtpose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedResidualBlock2(nn.Module):\n",
    "    def __init__(self, in_features, out_features, n_layers=5):\n",
    "        super(ExtendedResidualBlock2, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.fc1 = nn.Linear(in_features, out_features)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_features != out_features:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Linear(in_features, out_features))\n",
    "\n",
    "        self.fcn_dict = nn.ModuleDict()\n",
    "        self.fcn_dict2 = nn.ModuleDict()\n",
    "        for i in range(n_layers):\n",
    "            self.fcn_dict[str(i)] = nn.Sequential(nn.Linear(out_features, out_features))\n",
    "         \n",
    "    def forward(self, x):\n",
    "        out = F.tanh(self.fc1(x)) \n",
    "        for i in range(self.n_layers):\n",
    "            out = self.fcn_dict[str(i)](out)\n",
    "            if (i+1)%3==0:\n",
    "                out = out + self.shortcut(x)\n",
    "        out = out + self.shortcut(x)\n",
    "        return out\n",
    "\n",
    "class EfficientNetwork2(nn.Module):\n",
    "    def __init__(self, inp, out, n_layers=5):\n",
    "        super(EfficientNetwork2, self).__init__()\n",
    "        self.input_layer = nn.Linear(inp, 1024)\n",
    "        self.residual_block1 = ExtendedResidualBlock2(1024, 512, n_layers)\n",
    "        self.residual_block2 = ExtendedResidualBlock2(512, 256, n_layers)\n",
    "        self.residual_block3 = ExtendedResidualBlock2(256, 128, n_layers)\n",
    "        self.residual_block4 = ExtendedResidualBlock2(128, 64, n_layers)\n",
    "        self.residual_block5 = ExtendedResidualBlock2(64, 32, n_layers)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.output_layer = nn.Linear(16, out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.input_layer(x)\n",
    "        out = self.residual_block1(out)\n",
    "        out = self.residual_block2(out)\n",
    "        out = self.residual_block3(out)\n",
    "        out = self.residual_block4(out)\n",
    "        out = self.residual_block5(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModResNet2(nn.Module):\n",
    "    def __init__(self, in_chans, out):\n",
    "        super(ModResNet2, self).__init__()\n",
    "        original_model = models.resnet101(pretrained=True)\n",
    "        original_model.conv1 = nn.Conv2d(\n",
    "                    in_channels=in_chans,  # Change from 3 to 1 to accept grayscale images\n",
    "                    out_channels=original_model.conv1.out_channels,\n",
    "                    kernel_size=original_model.conv1.kernel_size,\n",
    "                    stride=original_model.conv1.stride,\n",
    "                    padding=original_model.conv1.padding,\n",
    "                    bias=original_model.conv1.bias)\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            original_model.conv1,\n",
    "            original_model.bn1,\n",
    "            original_model.relu,\n",
    "            original_model.maxpool,\n",
    "            original_model.layer1,\n",
    "            original_model.layer2,\n",
    "            original_model.layer3,\n",
    "            original_model.layer4\n",
    "        )\n",
    "        self.avgpool = original_model.avgpool\n",
    "        num_features = original_model.fc.in_features\n",
    "        num_out_feas = out\n",
    "        original_model.fc = nn.Linear(num_features, num_out_feas)\n",
    "        self.fc = original_model.fc\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        out_fc = self.fc(x)\n",
    "        return out_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiamesePoseNet3b_dec(nn.Module):\n",
    "    def __init__(self): \n",
    "        super(SiamesePoseNet3b_dec, self).__init__()\n",
    "        self.model = ModResNet2(1,512)\n",
    "        self.lin4c = EfficientNetwork2(512, 4, 2)     \n",
    "        self.lin4d = EfficientNetwork2(512, 3, 2)\n",
    "    def forward(self, rgbd1, rgbd2):\n",
    "        f1_rgb, f2_rgb = self.model(rgbd1), self.model(rgbd2)\n",
    "        B1, D1 = f1_rgb.shape\n",
    "        B2, D2 = f2_rgb.shape\n",
    "        f1_rgb = f1_rgb.unsqueeze(1)\n",
    "        f2_rgb = f2_rgb.unsqueeze(0)\n",
    "        out_prod = f1_rgb.expand(B1, B2, D1) - f2_rgb.expand(B1, B2, D2)\n",
    "        p_wxyz = self.lin4c(out_prod)\n",
    "        p_xyz = self.lin4d(out_prod)\n",
    "        return p_wxyz, p_xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1_e = SiamesePoseNet3b_dec().to(device)\n",
    "mod1_e.load_state_dict(torch.load('./gbdtposenet.pth'))\n",
    "mod1_e = mod1_e.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb_rgbd_data(img, sz=300):\n",
    "    \"\"\"img0 is gray scale image\n",
    "       If using ViT backbone, set sz=224\"\"\"\n",
    "    img = np.array(img)\n",
    "    img = img.astype('float32')\n",
    "    img = img/255.0 \n",
    "    img = F.interpolate(torch.tensor(img).unsqueeze(0).unsqueeze(0), size=(sz,sz), mode='bilinear', align_corners=False)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_rgb_inputs(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    rgbd1_tsr = comb_rgbd_data(img).float()\n",
    "    return rgbd1_tsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temp_feas(mod1, img_list2):\n",
    "    ### This function returns the features of all template images offline\n",
    "    with torch.no_grad():\n",
    "        all_feas = []\n",
    "        for i in range(len(img_list2)):\n",
    "            rgbd2_i = prep_rgb_inputs(img_list2[i])\n",
    "            f_temp_i = mod1.model(rgbd2_i.to(device))\n",
    "            all_feas.append(f_temp_i) \n",
    "        all_feas_tnsr = torch.vstack(all_feas) \n",
    "    return all_feas_tnsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_to_matrix2(qv):\n",
    "    qx, qy, qz, qw = qv[:,1].reshape(-1,1), qv[:,2].reshape(-1,1), qv[:,3].reshape(-1,1), qv[:,0].reshape(-1,1)\n",
    "    pred_q_ck_ci = np.hstack((qx, qy, qz, qw))\n",
    "    l2_norm = np.linalg.norm(pred_q_ck_ci, axis=1, keepdims=True)\n",
    "    pred_q_ck_ci = pred_q_ck_ci/l2_norm\n",
    "    pred_R_ck_ci = R.from_quat(pred_q_ck_ci).as_matrix()\n",
    "    pred_R_ci_ck = pred_R_ck_ci.transpose(0,2,1)\n",
    "    return pred_R_ci_ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_rots(qv, thr=25, nu=False):\n",
    "    \"\"\"qv is Nx4 numpy array of quartenion in qw, qx, qy, qz\"\"\"\n",
    "    ind1 = np.arange(len(qv)) # \n",
    "    qx, qy, qz, qw = qv[:,1].reshape(-1,1), qv[:,2].reshape(-1,1), qv[:,3].reshape(-1,1), qv[:,0].reshape(-1,1)\n",
    "    pred_q_ck_ci = np.hstack((qx, qy, qz, qw))\n",
    "    l2_norm = np.linalg.norm(pred_q_ck_ci, axis=1, keepdims=True)\n",
    "    pred_q_ck_ci = pred_q_ck_ci/l2_norm\n",
    "    pred_eul_ck_ci = R.from_quat(pred_q_ck_ci).as_euler('xyz', degrees=True) #Nx3 euler angles\n",
    "    pred_R_ck_ci = R.from_quat(pred_q_ck_ci).as_matrix()\n",
    "    pred_R_ci_ck = pred_R_ck_ci.transpose(0,2,1)\n",
    "    p_ind = np.all(np.abs(pred_eul_ck_ci) <= thr, axis=1)\n",
    "    if nu:\n",
    "        pred_eul_ck_ci = pred_eul_ck_ci[p_ind, :]\n",
    "        pred_R_ci_ck = pred_R_ci_ck[p_ind, :, :]\n",
    "    return p_ind, pred_R_ci_ck, pred_eul_ck_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_val(val):\n",
    "    id1 = val >= 90\n",
    "    id2 = val <= -90\n",
    "    val[id1] = 180 - val[id1]\n",
    "    val[id2] = -180 - val[id2]\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global R_c_b ## If Gazebo is used, R_c_b is the rotation matrix from the camera to body frame of the UAV\n",
    "roll_c_u, pitch_c_u, yaw_c_u = -(3.141592/2), 0, 3.141592/2\n",
    "rx = np.matrix([[1, 0, 0],[0, math.cos(roll_c_u),-math.sin(roll_c_u)], [0, math.sin(roll_c_u), math.cos(roll_c_u)]])\n",
    "ry = np.matrix([[math.cos(pitch_c_u), 0, math.sin(pitch_c_u)],[0, 1, 0],[-math.sin(pitch_c_u), 0, math.cos(pitch_c_u)]])\n",
    "rz = np.matrix([[math.cos(yaw_c_u), -math.sin(yaw_c_u), 0],[math.sin(yaw_c_u), math.cos(yaw_c_u), 0],[0, 0, 1]])\n",
    "R_c_b = np.matmul(np.matmul(rz,ry),rx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "## load the template images, their translations wrt G and quaternions denoting rotations from G to the camera frame\n",
    "path_to_temp_imgs = './...'\n",
    "path_to_temp_translations = './...'\n",
    "path_to_temp_quaternions = './...'\n",
    "T2_list = np.load(path_to_temp_translations).tolist()\n",
    "Q2_list = np.load(path_to_temp_quaternions).tolist()\n",
    "img2_list = []\n",
    "for i in range(len(T2_list)):\n",
    "    img2_list.append(cv2.imread(path_to_temp_imgs+'temp_'+str(i)+'.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_temps = create_temp_feas(mod1_e, img2_list)\n",
    "out_temps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"This function computes the softmax score of a source image pose wrt all template images for weighting the pose proposals\"\"\"\n",
    "    x_shifted = x - np.max(x, axis=0, keepdims=True)\n",
    "    exp_x = np.exp(x_shifted)\n",
    "    return exp_x / np.sum(exp_x, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_trans(tv, thr=2.5):\n",
    "    p_ind = np.all(np.abs(tv)<= thr, axis=1).tolist()\n",
    "    return p_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_eval360(rgbd1_tsr, p_gt, mod1):\n",
    "    modn = mod1.eval()\n",
    "    gt_T2_tsr, gt_Q2_tsr = p_gt\n",
    "    f1_rgb = modn.model(rgbd1_tsr.to(device))\n",
    "    f2_rgb = out_temps\n",
    "    B1, D1 = f1_rgb.shape\n",
    "    B2, D2 = f2_rgb.shape\n",
    "    f1_rgb = f1_rgb.unsqueeze(1)\n",
    "    f2_rgb = f2_rgb.unsqueeze(0)\n",
    "    out_prod = f1_rgb.expand(B1, B2, D1) - f2_rgb.expand(B1, B2, D2)\n",
    "    pred_Q_rel_ck_ci_tsr = modn.lin4c(out_prod)\n",
    "    pred_T_rel_ck_ci_tsr = modn.lin4d(out_prod)\n",
    "\n",
    "    N1 = rgbd1_tsr.shape[0] ## If multiple cameras are used at once, then N1 > 1 otherwise, N1 = 1\n",
    "    feas_temps = f2_rgb.squeeze(0).detach().cpu().numpy()\n",
    "    all_pose_comb = []\n",
    "    all_pd_trans, all_pd_rot = [], []\n",
    "    all_t_unc, all_r_unc = [], []\n",
    "    for i in range(N1):\n",
    "        pred_qi_ck_ci = pred_Q_rel_ck_ci_tsr[i, :, 0:4].detach().cpu().numpy()\n",
    "        pred_ti_ck_ci = pred_T_rel_ck_ci_tsr[i, :, 0:3].detach().cpu().numpy()\n",
    "        pind_1 = valid_trans(pred_ti_ck_ci)\n",
    "        pind_2, pred_Ri_ci_ck, pred_eul_ck_ci = valid_rots(pred_qi_ck_ci)\n",
    "        pind = (np.array(pind_1)*np.array(pind_2)).tolist()\n",
    "        if np.any(pind):\n",
    "            N = sum(pind)\n",
    "            pred_Ri_ci_ck, pred_eul_ck_ci = pred_Ri_ci_ck[pind, :, :], pred_eul_ck_ci[pind, :]\n",
    "            pred_ti_ck_ci = pred_ti_ck_ci[pind, :]\n",
    "            gt_Q2_tsr_i = gt_Q2_tsr[pind, :].cpu().numpy()\n",
    "            gt_R_g_ck = quaternion_to_matrix2(gt_Q2_tsr_i).transpose(0,2,1)\n",
    "            gt_T2_tsr_i = gt_T2_tsr[pind, :]\n",
    "            T_g_ck = gt_T2_tsr_i.cpu().numpy()\n",
    "            pred_R_g_ci = np.matmul(pred_Ri_ci_ck.transpose(0,2,1), gt_R_g_ck)\n",
    "            pred_T_ck_ci = pred_ti_ck_ci[:,:,np.newaxis]\n",
    "            R_c_b_rep = np.stack([np.array(R_c_b)]*N)\n",
    "            pred_R_bi_g = np.matmul(pred_R_g_ci.transpose(0,2,1), R_c_b_rep.transpose(0,2,1))\n",
    "            p_pred_rpy_bi_g = clip_val(np.array(R.from_matrix(pred_R_bi_g).as_euler('xyz', degrees=True)))\n",
    "            gt_R_ck_g = gt_R_g_ck.transpose(0,2,1)\n",
    "            p_pred_T_g_ci = T_g_ck + (np.matmul(gt_R_ck_g, pred_T_ck_ci)).squeeze(2)\n",
    "        \n",
    "            feas_i = f1_rgb.squeeze(1)[i, :].detach().cpu().numpy().reshape(1,-1)\n",
    "            feas_2 = feas_temps[pind,:]\n",
    "            cs_sim = np.dot(feas_2, feas_i.T) / (np.linalg.norm(feas_i) * np.linalg.norm(feas_2, axis=1, keepdims=True))\n",
    "            wt_i = softmax(cs_sim.reshape(-1,1))\n",
    "\n",
    "            if len(p_pred_T_g_ci)>=1:\n",
    "                all_pose_comb.append(np.hstack((p_pred_T_g_ci, p_pred_rpy_bi_g)))\n",
    "                pred_T_g_ci_val = np.nansum(wt_i*p_pred_T_g_ci, 0).flatten()\n",
    "                pred_rpy_bi_g_val = np.nansum(wt_i*p_pred_rpy_bi_g, 0).flatten()\n",
    "                trans_unc = np.var(p_pred_T_g_ci, 0)\n",
    "                rot_unc = np.var(p_pred_rpy_bi_g, axis=0)\n",
    "                all_t_unc.append(trans_unc), all_r_unc.append(rot_unc)\n",
    "                all_pd_rot.append(pred_rpy_bi_g_val)\n",
    "                all_pd_trans.append(pred_T_g_ci_val)\n",
    "\n",
    "    if len(all_pd_rot)>=1:\n",
    "        rots = np.array(all_pd_rot)\n",
    "        trans = np.array(all_pd_trans)\n",
    "        rots_unc, tran_unc = np.array(all_r_unc), np.array(all_t_unc)\n",
    "        return rots, trans, rots_unc, tran_unc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global T2_main_tsr, Q2_main_tsr\n",
    "T2_main_tsr = torch.tensor(T2_list).float().to(device)\n",
    "Q2_main_tsr = torch.tensor(Q2_list).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform2enu(x_g, rot_g, t_unc_g, rot_unc_g):\n",
    "    \"\"\"T_enu_g is the tranformation from the frame G to the enu frame for drone navigation with Ardupilot\"\"\"\n",
    "    R_g_enu = np.zeros((3,3)) \n",
    "    R_g_enu[0,1] = 1.0\n",
    "    R_g_enu[1,0] = -1.0\n",
    "    R_g_enu[2,2] = 1.0\n",
    "    R_b_g = R.from_euler('xyz', rot_g, degrees=True).as_matrix()\n",
    "    x_enu = R_g_enu@x_g.reshape(-1,1)\n",
    "    quat_enu = R.from_matrix((R_g_enu@R_b_g).T).as_quat()\n",
    "    cov_trans_enu = np.diag((np.abs(R_g_enu@t_unc_g.reshape(-1,1))).flatten())\n",
    "    cov_rot_enu = np.diag((np.abs(R_g_enu@rot_unc_g.reshape(-1,1))).flatten())\n",
    "    zeros_3x3 = np.zeros_like(cov_rot_enu)\n",
    "    cov_enu = np.block([[cov_trans_enu, zeros_3x3],[zeros_3x3, cov_rot_enu]])\n",
    "    return x_enu.flatten(), quat_enu.flatten(), cov_enu.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sensor_msgs.msg import Image\n",
    "from sensor_msgs.msg import CameraInfo\n",
    "from rospy.numpy_msg import numpy_msg\n",
    "from nav_msgs.msg import Odometry\n",
    "from geometry_msgs.msg import PoseStamped\n",
    "from geometry_msgs.msg import PoseWithCovarianceStamped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_img = None    \n",
    "def real_imcallback(msg):\n",
    "    global real_img\n",
    "    real_img = np.frombuffer(msg.data, dtype=np.uint8).reshape(msg.height, msg.width, -1)\n",
    "## Subscribe to T265 fisheye camera\n",
    "\n",
    "#TODO\n",
    "camera_topic = \"/camera/fisheye1/image_raw\" # provide the camera topic name here\n",
    "real_uav_img = rospy.Subscriber(\"/camera/fisheye1/image_raw\", numpy_msg(Image), callback = real_imcallback)\n",
    "\n",
    "use_ekf = False ## If integration with T265 is needed, set this parameter to True\n",
    "if use_ekf:\n",
    "    real_uav_odom = rospy.Publisher(\"/gbdtpose\", Odometry, queue_size=10)\n",
    "else:\n",
    "    real_uav_odom = rospy.Publisher(\"/mavros/vision_pose/pose_cov\", PoseWithCovarianceStamped, queue_size=10)\n",
    "\n",
    "real_odom = PoseWithCovarianceStamped()\n",
    "rate = rospy.Rate(30) ## Normal rate for transmitting poses to the UAV flight controller\n",
    "while not(rospy.is_shutdown()):\n",
    "    real_img = cv2.imread('./tempfisheye2_360_1/temp_0.png')\n",
    "    if (real_img is not None):\n",
    "        rgbd1_tsr_src = prep_rgb_inputs(real_img)    \n",
    "        p_gt = (T2_main_tsr.to(device), Q2_main_tsr.to(device))   \n",
    "        out3 = global_eval360(rgbd1_tsr_src, p_gt, mod1_e)\n",
    "        rots, trans, rot_unc, t_unc = out3\n",
    "        xyz_enu, quat_enu, cov_enu = transform2enu(trans.flatten(), rots.flatten(), t_unc.flatten(), rot_unc.flatten())         \n",
    "        real_odom.header.stamp = rospy.Time.now()\n",
    "        real_odom.pose.pose.position.x = xyz_enu[0]\n",
    "        real_odom.pose.pose.position.y = xyz_enu[1]\n",
    "        real_odom.pose.pose.position.z = xyz_enu[2]\n",
    "        real_odom.pose.pose.orientation.x = quat_enu[0]\n",
    "        real_odom.pose.pose.orientation.y = quat_enu[1]\n",
    "        real_odom.pose.pose.orientation.z = quat_enu[2]\n",
    "        real_odom.pose.pose.orientation.w = quat_enu[3]\n",
    "        real_odom.pose.covariance = cov_enu.tolist()\n",
    "        real_uav_odom.publish(real_odom)\n",
    "    rate.sleep()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
