{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import torchvision.models as models\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quat_to_R_matrix(q):\n",
    "    q = q / q.norm(p=2)\n",
    "    q_w, q_x, q_y, q_z = q\n",
    "    Rmat = torch.stack([\n",
    "        torch.stack([1 - 2*(q_y**2 + q_z**2), 2*(q_x*q_y - q_w*q_z), 2*(q_x*q_z + q_w*q_y)]),\n",
    "        torch.stack([2*(q_x*q_y + q_w*q_z), 1 - 2*(q_x**2 + q_z**2), 2*(q_y*q_z - q_w*q_x)]),\n",
    "        torch.stack([2*(q_x*q_z - q_w*q_y), 2*(q_y*q_z + q_w*q_x), 1 - 2*(q_x**2 + q_y**2)])])\n",
    "    return Rmat.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_gt_relpose_360(T1_ls, Q1_ls, T2_ls, Q2_ls, diff_t=2.5, diff_r=25.0):\n",
    "    \"\"\"This function computes the ground truth realtive translations of the source images wrt template images\n",
    "       together with their normalized relative quartenions\n",
    "       T1_ls is a list of source image translations wrt G\n",
    "       Q1_ls is a list of source image quats. wrt G\n",
    "       T2_ls is a list of template image translations wrt G\n",
    "       Q2_ls is a list of template image quats. wrt G\n",
    "       diff_t is the maximum allowable relative translation between source and template image\n",
    "       diff_r is the maximum allowable relative rotation (in euler angles) between source and template image\n",
    "       \"\"\"\n",
    "    T_rel_k_i_list = []\n",
    "    Q_rel_ck_ci_list = []\n",
    "    flags = []\n",
    "    for i in range(len(T1_ls)):\n",
    "        T_i = np.array(T1_ls[i]).reshape(-1,1)\n",
    "        R_i = quat_to_R_matrix(torch.tensor(Q1_ls[i])).numpy()\n",
    "        Trel_i = []\n",
    "        Qrel_i = []\n",
    "        fl_i = []\n",
    "        for j in range(len(T2_ls)):\n",
    "            T_k = np.array(T2_ls[j]).reshape(-1,1)\n",
    "            R_k = quat_to_R_matrix(torch.tensor(Q2_ls[j])).numpy()\n",
    "            Trel_k_i = (R_k)@(T_i - T_k)\n",
    "            fl_t = np.all(np.abs(Trel_k_i.flatten())<=diff_t).item()\n",
    "            R_rel_ck_ci = R_i@R_k.T\n",
    "            q_rel_ck_ci = R.from_matrix(R_rel_ck_ci).as_quat()\n",
    "            eul_rel_ck_ci = R.from_matrix(R_rel_ck_ci).as_euler('xyz', degrees=True)\n",
    "            fl_r = np.all(np.abs(eul_rel_ck_ci.flatten())<=diff_r).item()\n",
    "            fl_i.append(fl_t*fl_r)\n",
    "            \n",
    "            q_rel_ck_ci = torch.tensor([q_rel_ck_ci[3], q_rel_ck_ci[0], q_rel_ck_ci[1], q_rel_ck_ci[2]])\n",
    "            q_rel_ck_ci = q_rel_ck_ci / q_rel_ck_ci.norm(p=2) # unit norm quarternion\n",
    "            Trel_i.append(Trel_k_i.flatten().tolist())\n",
    "            Qrel_i.append(q_rel_ck_ci.flatten().tolist())\n",
    "            \n",
    "        T_rel_k_i_list.append(Trel_i)\n",
    "        Q_rel_ck_ci_list.append(Qrel_i)\n",
    "        flags.append(fl_i)\n",
    "  \n",
    "    T_rel_ck_ci = torch.tensor(T_rel_k_i_list).float().to(device) ## NxMx3 ground truth relative translation of source wrt target\n",
    "    Q_rel_ck_ci = torch.tensor(Q_rel_ck_ci_list).float().to(device) ## NxMx4 ground truth relative quartenions of source wrt target\n",
    "    fl_ci_ck = torch.tensor(flags).bool().to(device) ## NxM flags of valid relative poses\n",
    "    return T_rel_ck_ci, Q_rel_ck_ci, fl_ci_ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geom_cstr_comb_flags360(T1_list, Q1_list, T2_list, Q2_list, diff_t=2.5, diff_r=25.0): \n",
    "    \"\"\"returns the flags for valid camera pairs within translation and rotation limits\"\"\"\n",
    "    all_Q_list = Q1_list[:]\n",
    "    all_Q_list.extend(Q2_list)\n",
    "    all_T_list = T1_list[:]\n",
    "    all_T_list.extend(T2_list)\n",
    "    geom_flag = []\n",
    "    for i in range(len(all_Q_list)):\n",
    "        flag2_i = []\n",
    "        T1 = np.array(all_T_list[i]).reshape(-1,1)\n",
    "        q1 = all_Q_list[i]\n",
    "        R1 = np.array((R.from_quat([q1[1],q1[2],q1[3],q1[0]])).as_matrix())\n",
    "        for j in range(len(all_Q_list)):\n",
    "            T2_j = np.array(all_T_list[j]).reshape(-1,1)\n",
    "            q2 = all_Q_list[j]\n",
    "            R2_j = np.array((R.from_quat([q2[1],q2[2],q2[3],q2[0]])).as_matrix())\n",
    "            Trel_k_i = (R2_j)@(T1 - T2_j)\n",
    "            fl_t = np.all(np.abs(Trel_k_i.flatten())<=diff_t).item()\n",
    "            R_rel_ck_ci = R1@R2_j.T\n",
    "            eul_rel_ck_ci = R.from_matrix(R_rel_ck_ci).as_euler('xyz', degrees=True)\n",
    "            fl_r = np.all(np.abs(eul_rel_ck_ci.flatten())<=diff_r).item()\n",
    "            flag2_i.append(fl_t*fl_r)  \n",
    "        geom_flag.append(flag2_i)\n",
    "    return torch.tensor(geom_flag).bool().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cep_err(arr1, arr2):\n",
    "    err = np.nanmedian(np.abs(arr1 - arr2))\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_val(val):\n",
    "    id1 = val >= 90\n",
    "    id2 = val <= -90\n",
    "    val[id1] = 180 - val[id1]\n",
    "    val[id2] = -180 - val[id2]\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pose_loss360(pred_Q_rel_ck_ci_tsr, pred_T_rel_ck_ci_tsr, flag, gt_Q_rel_ck_ci_tsr, gt_T_rel_k_i_tsr, \n",
    "                        criterion, l1=1e4, l2=1e2):\n",
    "    quat_loss = []\n",
    "    trans_loss = []\n",
    "    for i in range(gt_Q_rel_ck_ci_tsr.shape[0]):\n",
    "        for k in range(gt_Q_rel_ck_ci_tsr.shape[1]):\n",
    "            if flag[i, k].item():\n",
    "                pred_q_rel_ck_ci = pred_Q_rel_ck_ci_tsr[i,k,0:4]\n",
    "                pred_q_rel_ck_ci = pred_q_rel_ck_ci/pred_q_rel_ck_ci.norm(p=2)\n",
    "                gt_q_i = gt_Q_rel_ck_ci_tsr[i,k,:]\n",
    "                quat_loss.append(l1*criterion(pred_q_rel_ck_ci, gt_q_i))\n",
    "                pred_T_rel_ck_ci = pred_T_rel_ck_ci_tsr[i,k,:]\n",
    "                gt_T_rel_k_i = gt_T_rel_k_i_tsr[i,k,:]\n",
    "                trans_loss.append(l2*criterion(pred_T_rel_ck_ci, gt_T_rel_k_i))\n",
    "    if len(quat_loss)>=1:\n",
    "        ql = sum(quat_loss)/len(quat_loss)\n",
    "    if len(trans_loss)>=1:\n",
    "        tl = sum(trans_loss)/len(trans_loss)\n",
    "    return ql, tl     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb_rgbd_data(img0, sz=300):\n",
    "    \"\"\"img0 is gray scale image\n",
    "       If using ViT backbone, set sz=224\"\"\"\n",
    "    img = cv2.cvtColor(img0, cv2.COLOR_BGR2GRAY)\n",
    "    img = np.array(img)\n",
    "    img = img.astype('float32')\n",
    "    img = img/255.0 \n",
    "    img = F.interpolate(torch.tensor(img).unsqueeze(0).unsqueeze(0), size=(sz,sz), mode='bilinear', align_corners=False)\n",
    "    img = img.squeeze(0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_inputs3(img1_list, img2_list):\n",
    "    rgbd1_tsr_list_src, rgbd1_tsr_list_temp = [], []\n",
    "    for i in range(len(img1_list)): \n",
    "        rgbd1_tsr_src = comb_rgbd_data(img1_list[i]) \n",
    "        rgbd1_tsr_list_src.append(rgbd1_tsr_src)\n",
    "    for j in range(len(img2_list)):\n",
    "        rgbd1_tsr_temp = comb_rgbd_data(img2_list[j])\n",
    "        rgbd1_tsr_list_temp.append(rgbd1_tsr_temp)\n",
    "    rgbd1_tsr_src = torch.stack(rgbd1_tsr_list_src,0).float().to(device)\n",
    "    rgbd1_tsr_temp = torch.stack(rgbd1_tsr_list_temp,0).float().to(device)\n",
    "    return rgbd1_tsr_src, rgbd1_tsr_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedResidualBlock2(nn.Module):\n",
    "    def __init__(self, in_features, out_features, n_layers=5):\n",
    "        super(ExtendedResidualBlock2, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.fc1 = nn.Linear(in_features, out_features)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_features != out_features:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Linear(in_features, out_features))\n",
    "\n",
    "        self.fcn_dict = nn.ModuleDict()\n",
    "        self.fcn_dict2 = nn.ModuleDict()\n",
    "        for i in range(n_layers):\n",
    "            self.fcn_dict[str(i)] = nn.Sequential(nn.Linear(out_features, out_features))\n",
    "         \n",
    "    def forward(self, x):\n",
    "        out = F.tanh(self.fc1(x))\n",
    "        for i in range(self.n_layers):\n",
    "            out = self.fcn_dict[str(i)](out)\n",
    "            if (i+1)%3==0:\n",
    "                out = out + self.shortcut(x)\n",
    "        out = out + self.shortcut(x)\n",
    "        return out\n",
    "\n",
    "class EfficientNetwork2(nn.Module):\n",
    "    def __init__(self, inp, out, n_layers=5):\n",
    "        super(EfficientNetwork2, self).__init__()\n",
    "        self.input_layer = nn.Linear(inp, 1024)\n",
    "        self.residual_block1 = ExtendedResidualBlock2(1024, 512, n_layers)\n",
    "        self.residual_block2 = ExtendedResidualBlock2(512, 256, n_layers)\n",
    "        self.residual_block3 = ExtendedResidualBlock2(256, 128, n_layers)\n",
    "        self.residual_block4 = ExtendedResidualBlock2(128, 64, n_layers)\n",
    "        self.residual_block5 = ExtendedResidualBlock2(64, 32, n_layers)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.output_layer = nn.Linear(16, out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.input_layer(x)\n",
    "        out = self.residual_block1(out)\n",
    "        out = self.residual_block2(out)\n",
    "        out = self.residual_block3(out)\n",
    "        out = self.residual_block4(out)\n",
    "        out = self.residual_block5(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.output_layer(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModResNet2(nn.Module):\n",
    "    def __init__(self, in_chans, out):\n",
    "        super(ModResNet2, self).__init__()\n",
    "        original_model = models.resnet101(pretrained=True)\n",
    "        \n",
    "        original_model.conv1 = nn.Conv2d(\n",
    "                    in_channels=in_chans,  # Change from 1 to 3 to accept rgb images\n",
    "                    out_channels=original_model.conv1.out_channels,\n",
    "                    kernel_size=original_model.conv1.kernel_size,\n",
    "                    stride=original_model.conv1.stride,\n",
    "                    padding=original_model.conv1.padding,\n",
    "                    bias=original_model.conv1.bias)\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            original_model.conv1,\n",
    "            original_model.bn1,\n",
    "            original_model.relu,\n",
    "            original_model.maxpool,\n",
    "            original_model.layer1,\n",
    "            original_model.layer2,\n",
    "            original_model.layer3,\n",
    "            original_model.layer4\n",
    "        )\n",
    "        self.avgpool = original_model.avgpool\n",
    "        \n",
    "        num_features = original_model.fc.in_features\n",
    "        num_out_feas = out\n",
    "        original_model.fc = nn.Linear(num_features, num_out_feas)\n",
    "        self.fc = original_model.fc  \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        out_fc = self.fc(x)\n",
    "        return out_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vit_b_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomViT(nn.Module):\n",
    "    def __init__(self, num_channels=1, output_size=512):\n",
    "        super(CustomViT, self).__init__()\n",
    "        self.vit = vit_b_16(pretrained=True)\n",
    "        self.vit.conv_proj = nn.Conv2d(num_channels, self.vit.conv_proj.out_channels,\n",
    "                                       kernel_size=self.vit.conv_proj.kernel_size,\n",
    "                                       stride=self.vit.conv_proj.stride,\n",
    "                                       padding=self.vit.conv_proj.padding,\n",
    "                                       bias=False)\n",
    "        self.vit.heads = nn.Linear(self.vit.heads.head.in_features, 512)\n",
    "    def forward(self, x):\n",
    "        out = self.vit(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiamesePoseNet3b_dec(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiamesePoseNet3b_dec, self).__init__()\n",
    "        self.model = ModResNet2(1,512) #CustomViT() --- use this for vision transformer\n",
    "        self.lin4c = EfficientNetwork2(512, 4, 2)     \n",
    "        self.lin4d = EfficientNetwork2(512, 3, 2)\n",
    "    def forward(self, rgbd1, rgbd2):\n",
    "        f1_rgb, f2_rgb = self.model(rgbd1), self.model(rgbd2)\n",
    "        B1, D1 = f1_rgb.shape\n",
    "        B2, D2 = f2_rgb.shape\n",
    "        f1_rgb = f1_rgb.unsqueeze(1)\n",
    "        f2_rgb = f2_rgb.unsqueeze(0)\n",
    "        out_prod = f1_rgb.expand(B1, B2, D1) - f2_rgb.expand(B1, B2, D2)\n",
    "        q_wxyz = self.lin4c(out_prod) ## NxMx4 unnormalized quartenions predictions\n",
    "        xyz = self.lin4d(out_prod) ## NxMx3 relative translation predictions\n",
    "        return q_wxyz, xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_inter_rot(i, k, pred_Q_rel_ck_ci_tsr, gt_Q1_tsr, gt_Q2_tsr, N1):\n",
    "    # returns the intermediate rotation matrix i.e R_ck_ci\n",
    "    ## i stands for the right index while k stands for the left index\n",
    "    ## an index value less than 10 implies domain 1 else its domain 2\n",
    "    N1_b = gt_Q1_tsr.shape[0]\n",
    "    N2 = pred_Q_rel_ck_ci_tsr.shape[1]\n",
    "    if i<N1 and k>=N1:\n",
    "        R_ck_ci = quat_to_R_matrix(pred_Q_rel_ck_ci_tsr[i, k-N1, 0:4]) \n",
    "    elif i<N1 and k<N1: # both cameras are in domain 1\n",
    "        rind = np.random.randint(0,N2)\n",
    "        r_r = quat_to_R_matrix(pred_Q_rel_ck_ci_tsr[k, rind, 0:4])\n",
    "        r_l = quat_to_R_matrix(pred_Q_rel_ck_ci_tsr[i, rind, 0:4])\n",
    "        R_ck_ci = r_l@r_r.T # rotation from ck to ci\n",
    "    elif i>=N1 and k>=N1: # both cameras are in domain 2\n",
    "        r_r = quat_to_R_matrix(gt_Q2_tsr[k-N1, :])\n",
    "        r_l = quat_to_R_matrix(gt_Q2_tsr[i-N1, :])\n",
    "        R_ck_ci = r_l@r_r.T\n",
    "    elif i>=N1 and k<N1: #camera i is in domain 2, but k in 1\n",
    "        pred_R_ci_ck = quat_to_R_matrix(pred_Q_rel_ck_ci_tsr[k, i-N1, 0:4])\n",
    "        R_ck_ci = pred_R_ci_ck.T\n",
    "    return R_ck_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def composed_rot_loss(n_ins, gt_Q1_tsr, gt_Q2_tsr, gt_Q_rel_ck_ci_tsr, pred_Q_rel_ck_ci_tsr, all_comb_flag,\n",
    "                      criterion, l=1e3, Nsel=300):\n",
    "   \"\"\"this function computes the composed rotational loss, given the graph of camera connectivity\n",
    "      n_ins -- this is the number of inner rotation matrices in the composed rotations\n",
    "      \"\"\"\n",
    "   \n",
    "   ncams = pred_Q_rel_ck_ci_tsr.shape[0] + gt_Q2_tsr.shape[0] \n",
    "   ncams = np.arange(ncams)\n",
    "   np.random.shuffle(ncams)\n",
    "   ncams = ncams[0:10].tolist()\n",
    "   all_inds = list(range(len(ncams)))\n",
    "   perms = list(itertools.permutations(all_inds, n_ins))\n",
    "   comb_rot_loss = []\n",
    "   N1 = pred_Q_rel_ck_ci_tsr.shape[0]\n",
    "   for i in ncams:\n",
    "      for j in ncams:\n",
    "         gt_flag_ij = all_comb_flag[i,j].item()\n",
    "         if len(perms)>Nsel:\n",
    "            perms_sel = random.sample(perms, Nsel)\n",
    "         else:\n",
    "            perms_sel = perms[:]\n",
    "         if gt_flag_ij:\n",
    "            for perm_k in perms_sel:\n",
    "               lst_perm_k = list(perm_k)\n",
    "               ext_arr = lst_perm_k[:]\n",
    "               ext_arr.extend([i, j])\n",
    "               if not(np.all(np.array(ext_arr)<N1) or np.all(np.array(ext_arr)>=N1)):\n",
    "                  out_flag_1 = all_comb_flag[i,lst_perm_k[0]].item()\n",
    "                  out_flag_2 = all_comb_flag[j,lst_perm_k[-1]].item()\n",
    "                  if out_flag_1 and out_flag_2:\n",
    "                     gt_R_cj_ci = comp_inter_rot(i, j, pred_Q_rel_ck_ci_tsr, \n",
    "                                                   gt_Q1_tsr, gt_Q2_tsr, N1)\n",
    "                     left_rot_mat = comp_inter_rot(i, lst_perm_k[0], pred_Q_rel_ck_ci_tsr, \n",
    "                                                   gt_Q1_tsr, gt_Q2_tsr, N1)\n",
    "                     right_rot_mat = comp_inter_rot(lst_perm_k[-1], j, pred_Q_rel_ck_ci_tsr, \n",
    "                                                   gt_Q1_tsr, gt_Q2_tsr, N1)\n",
    "\n",
    "                     comp_rots = [gt_R_cj_ci, left_rot_mat, right_rot_mat]\n",
    "                     sub_all_flags = []\n",
    "                     int_rot_mats = []\n",
    "                     if len(lst_perm_k)>=2:\n",
    "                        for k in range(len(lst_perm_k)-1):\n",
    "                           sub_flags_i = all_comb_flag[lst_perm_k[k],lst_perm_k[k+1]]\n",
    "                           sub_all_flags.append(sub_flags_i)\n",
    "                           int_rot_mat = comp_inter_rot(lst_perm_k[k],lst_perm_k[k+1], pred_Q_rel_ck_ci_tsr, \n",
    "                                                      gt_Q1_tsr, gt_Q2_tsr, N1)\n",
    "                           int_rot_mats.append(int_rot_mat)\n",
    "                           comp_rots.append(int_rot_mat)\n",
    "                     else:\n",
    "                        sub_all_flags.append(True)\n",
    "                        int_rot_mats.append(torch.eye(3).float().to(device))\n",
    "                        \n",
    "                     contains_no_none = all(elem is not None for elem in comp_rots)\n",
    "                     if len(sub_all_flags)>=1 and all(sub_all_flags) and contains_no_none:\n",
    "                        R_cj_ck = right_rot_mat\n",
    "                        for int_rot_mat_k in int_rot_mats:\n",
    "                           R_cj_ck = int_rot_mat_k@R_cj_ck\n",
    "                        R_cj_ci = left_rot_mat@R_cj_ck\n",
    "                        comb_rot_loss.append(l*criterion(R_cj_ci, gt_R_cj_ci))\n",
    "\n",
    "   if len(comb_rot_loss)>=10:\n",
    "      return sum(comb_rot_loss)/len(comb_rot_loss)\n",
    "   else:\n",
    "      return 0.0                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_inter_trans(i, k, pred_Q_rel_ck_ci_tsr, pred_T_rel_ck_ci_tsr, gt_T1_tsr, gt_T2_tsr, gt_Q1_tsr, gt_Q2_tsr, N1, id=True):\n",
    "    # returns the intermediate rotation matrix i.e R_ck_ci\n",
    "    N2 = pred_T_rel_ck_ci_tsr.shape[1]\n",
    "    if i<N1 and k>=N1: # if camera i is in domain 1, but lst_perm_k[0] in 2\n",
    "        T_ck_ci = pred_T_rel_ck_ci_tsr[i, k-N1, :].reshape(-1,1)   \n",
    "    elif i<N1 and k<N1: # both cameras are in domain 1\n",
    "        rind = np.random.randint(0,N2)\n",
    "        R_g_ck = quat_to_R_matrix(pred_Q_rel_ck_ci_tsr[k, rind, 0:4])\n",
    "        T_g_ck = pred_T_rel_ck_ci_tsr[k, rind, 0:3].reshape(-1,1)\n",
    "        T_g_ci = pred_T_rel_ck_ci_tsr[i, rind, 0:3].reshape(-1,1)\n",
    "        T_ck_ci = R_g_ck@(T_g_ci - T_g_ck)  \n",
    "    elif i>=N1 and k>=N1: # both cameras are in domain 2\n",
    "        T_g_ck = gt_T2_tsr[k-N1, :].reshape(-1,1)\n",
    "        T_g_ci = gt_T2_tsr[i-N1, :].reshape(-1,1)\n",
    "        R_g_ck = quat_to_R_matrix(gt_Q2_tsr[k-N1, :])\n",
    "        T_ck_ci = R_g_ck@(T_g_ci - T_g_ck)\n",
    "    elif i>=N1 and k<N1: #camera i is in domain 2, but k in 1\n",
    "        T_ci_ck = pred_T_rel_ck_ci_tsr[k, i-N1, :].reshape(-1,1)\n",
    "        R_ci_ck = quat_to_R_matrix(pred_Q_rel_ck_ci_tsr[k, i-N1, 0:4])\n",
    "        T_ck_ci = -1.0*R_ci_ck@T_ci_ck  \n",
    "    return T_ck_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_R_trans(k, j, pred_Q_rel_ck_ci_tsr, gt_Q1_tsr, gt_Q2_tsr, N1):\n",
    "    N2 = pred_Q_rel_ck_ci_tsr.shape[1]\n",
    "    if j<N1 and k>=N1:\n",
    "        R_cj_ck = quat_to_R_matrix(pred_Q_rel_ck_ci_tsr[j, k-N1, 0:4]).T\n",
    "    elif j<N1 and k<N1:\n",
    "        rind = np.random.randint(0,N2)\n",
    "        r_g_ck = quat_to_R_matrix(pred_Q_rel_ck_ci_tsr[k, rind, 0:4])\n",
    "        r_g_cj = quat_to_R_matrix(pred_Q_rel_ck_ci_tsr[j, rind, 0:4])\n",
    "        R_cj_ck = r_g_ck@r_g_cj.T\n",
    "    elif j>=N1 and k>=N1:\n",
    "        r_g_ck = quat_to_R_matrix(gt_Q2_tsr[k-N1, :])\n",
    "        r_g_cj = quat_to_R_matrix(gt_Q2_tsr[j-N1, :])\n",
    "        R_cj_ck = r_g_ck@r_g_cj.T\n",
    "    elif j>=N1 and k<N1:\n",
    "        R_cj_ck = quat_to_R_matrix(pred_Q_rel_ck_ci_tsr[k, j-N1, 0:4])\n",
    "    return R_cj_ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def composed_trans_loss(gt_Q1_tsr, gt_Q2_tsr, gt_T_rel_k_i_tsr, gt_Q_rel_ck_ci_tsr, \n",
    "                        pred_Q_rel_ck_ci_tsr, pred_T_rel_ck_ci_tsr, gt_T1_tsr, gt_T2_tsr, \n",
    "                        all_comb_flag, criterion, l=10.0):\n",
    "    \n",
    "\n",
    "  ncams = pred_T_rel_ck_ci_tsr.shape[0] + gt_T2_tsr.shape[0] \n",
    "  ncams = np.arange(ncams)\n",
    "  np.random.shuffle(ncams)\n",
    "  comb_trans_loss = []\n",
    "  N1 = pred_T_rel_ck_ci_tsr.shape[0]\n",
    "  for i in ncams:\n",
    "    for j in ncams:\n",
    "      gt_flag_ij = all_comb_flag[i,j].item()\n",
    "      if (gt_flag_ij):\n",
    "        for k in ncams:\n",
    "          gt_flag_jk = all_comb_flag[j,k].item()\n",
    "          gt_flag_ik = all_comb_flag[i,k].item()\n",
    "          ext_arr = [i, j, k]\n",
    "          dom_flg = not(np.all(np.array(ext_arr)<N1) or np.all(np.array(ext_arr)>=N1))\n",
    "          if gt_flag_jk and gt_flag_ik and dom_flg:\n",
    "            pred_T_ck_ci = comp_inter_trans(i, k, pred_Q_rel_ck_ci_tsr, pred_T_rel_ck_ci_tsr, gt_T1_tsr, \n",
    "                              gt_T2_tsr, gt_Q1_tsr, gt_Q2_tsr, N1)\n",
    "            pred_T_ck_cj = comp_inter_trans(j, k, pred_Q_rel_ck_ci_tsr, pred_T_rel_ck_ci_tsr, gt_T1_tsr, \n",
    "                              gt_T2_tsr, gt_Q1_tsr, gt_Q2_tsr, N1)\n",
    "            pred_T_cj_ci = comp_inter_trans(i, j, pred_Q_rel_ck_ci_tsr, pred_T_rel_ck_ci_tsr, gt_T1_tsr, \n",
    "                              gt_T2_tsr, gt_Q1_tsr, gt_Q2_tsr, N1)\n",
    "            pred_R_cj_ck = comp_R_trans(k, j, pred_Q_rel_ck_ci_tsr, gt_Q1_tsr, \n",
    "                                        gt_Q2_tsr, N1)\n",
    "            calc_pred_T_ck_ci = pred_T_ck_cj + pred_R_cj_ck@pred_T_cj_ci\n",
    "\n",
    "            tr_loss = l*criterion(calc_pred_T_ck_ci, pred_T_ck_ci)\n",
    "            comb_trans_loss.append(tr_loss)     \n",
    "  if len(comb_trans_loss)>=10:\n",
    "    return sum(comb_trans_loss)/len(comb_trans_loss)\n",
    "  else:\n",
    "    return 0.0               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(mod1, optimz, img_src_tsr, img_temp_tsr, T_rel_tsr, Q_rel_tsr, flags_tsr, gflag, T1_tsr, \n",
    "                  Q1_tsr, T2_tsr, Q2_tsr, criterion):\n",
    "    \n",
    "    pose_pred = mod1(img_src_tsr, img_temp_tsr)\n",
    "    pred_Q_rel_ck_ci_tsr = pose_pred[0]\n",
    "    pred_T_rel_ck_ci_tsr = pose_pred[1]\n",
    "    loss = loss_rot = loss_trns = 0.0\n",
    "\n",
    "    ### Compute data loss\n",
    "    loss_dat = compute_pose_loss360(pred_Q_rel_ck_ci_tsr, pred_T_rel_ck_ci_tsr, flags_tsr, \n",
    "                                     Q_rel_tsr, T_rel_tsr, criterion=criterion)\n",
    "    \n",
    "    loss_rot = loss_rot + loss_dat[0]\n",
    "    loss_trns = loss_trns + loss_dat[1]\n",
    "\n",
    "    ### Compute the geometric constraint loss --- the 1st argument is the number on intermediate poses\n",
    "    ### The last argument is the number of random samples as specified in the paper\n",
    "    g1_loss = composed_rot_loss(1, Q1_tsr, Q2_tsr, Q_rel_tsr, pred_Q_rel_ck_ci_tsr, gflag, criterion=criterion)\n",
    "    loss_rot = loss_rot + g1_loss\n",
    "    g2_loss = composed_rot_loss(2, Q1_tsr, Q2_tsr, Q_rel_tsr, pred_Q_rel_ck_ci_tsr, gflag, criterion=criterion, Nsel=20)\n",
    "    loss_rot = loss_rot + g2_loss\n",
    "    g3_loss = composed_rot_loss(3, Q1_tsr, Q2_tsr, Q_rel_tsr, pred_Q_rel_ck_ci_tsr, gflag, criterion=criterion, Nsel=20) \n",
    "    loss_rot = loss_rot + g3_loss\n",
    "    loss = loss + loss_rot\n",
    "    tr1_loss = composed_trans_loss(Q1_tsr, Q2_tsr, T_rel_tsr, Q_rel_tsr, pred_Q_rel_ck_ci_tsr, \n",
    "                                   pred_T_rel_ck_ci_tsr, T1_tsr, T2_tsr, gflag, criterion=criterion)\n",
    "    loss_trns = loss_trns + tr1_loss\n",
    "    loss = loss + loss_trns\n",
    "\n",
    "    \n",
    "    if torch.is_tensor(loss):   \n",
    "        optimz.zero_grad() \n",
    "        loss.backward()\n",
    "        optimz.step()   \n",
    "\n",
    "    with torch.no_grad():\n",
    "        pose_loses = [loss_dat[0], loss_dat[1], g1_loss, g2_loss, g3_loss, tr1_loss, loss_rot, loss_trns, loss]\n",
    "        for i, rot_loss in enumerate(pose_loses):\n",
    "            if torch.is_tensor(rot_loss):\n",
    "                pose_loses[i] = rot_loss.item()\n",
    "            else:\n",
    "                pose_loses[i] = None\n",
    "        return pose_loses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_progress(comb_loss):  \n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(18, 5))\n",
    "    for i in range(len(comb_loss[0])):\n",
    "        plt.subplot(1, len(comb_loss[0]), i+1)\n",
    "        l_i = []\n",
    "        for loss_k in comb_loss:\n",
    "            if loss_k[i] is not None:\n",
    "                l_i.append(loss_k[i]) \n",
    "        if len(l_i)>1:\n",
    "            plt.plot(l_i, label='\\nCur. loss: '+str(round(l_i[-1],5)))\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend()\n",
    "    plt.suptitle('Loss History')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion1 = nn.SmoothL1Loss().to(device)\n",
    "mod1 = SiamesePoseNet3b_dec()\n",
    "mod1.load_state_dict(torch.load('./resnet_backbone.pth'))\n",
    "mod1 = mod1.to(device).train()\n",
    "optimz = optim.Adam([\n",
    "    {'params': mod1.parameters(), 'lr': 1e-4},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprep(all_calc_p, all_calc_g):\n",
    "    pr, pp, py = [], [], []\n",
    "    gr, gp, gy = [], [], []\n",
    "    for p, g in zip(all_calc_p, all_calc_g):\n",
    "        pr.extend(p[:, 0].tolist()), pp.extend(p[:, 1].tolist()), py.extend(p[:, 2].tolist())\n",
    "        gr.extend(g[:, 0].tolist()), gp.extend(g[:, 1].tolist()), gy.extend(g[:, 2].tolist())\n",
    "    return (pr, gr), (pp, gp), (py, gy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repcep(dimx, dimy, dimz, nval):\n",
    "    er_r1 = calc_cep_err(np.array(dimx[0])[-nval:], np.array(dimx[-1])[-nval:])\n",
    "    er_p1 = calc_cep_err(np.array(dimy[0])[-nval:], np.array(dimy[-1])[-nval:])\n",
    "    er_y1 = calc_cep_err(np.array(dimz[0])[-nval:], np.array(dimz[-1])[-nval:])\n",
    "    rll_err = [er_r1]\n",
    "    ptc_err = [er_p1]\n",
    "    yw_err = [er_y1]\n",
    "    all_err = [rll_err, ptc_err, yw_err]\n",
    "    return all_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def plot_global_est2(all_calc, all_pred, all_gt, all_calc_t, all_pred_t, all_gt_t, \n",
    "                     all_rel_rot_pd, all_rel_rot_gt, all_rel_tr_pd, all_rel_tr_gt, nval2, \n",
    "                     all_global_rot_pd, all_global_rot_gt, all_global_t_pd, all_global_t_gt, \n",
    "                     nval3, nval4, c_all_gt_t_rel, c_all_pd_t_rel):\n",
    "    \n",
    "    if len(all_rel_rot_pd[-1])>=1 and len(all_global_rot_pd[-1])>=1:\n",
    "        rot_rell = reprep(all_rel_rot_pd, all_rel_rot_gt)\n",
    "        gb_rot = reprep(all_global_rot_pd, all_global_rot_gt)\n",
    "        gb_trans = reprep(all_global_t_pd, all_global_t_gt)\n",
    "        rot_err_rell = repcep(rot_rell[0], rot_rell[1], rot_rell[2], nval2)\n",
    "        gb_rot_err = repcep(gb_rot[0], gb_rot[1], gb_rot[2], nval3)\n",
    "        gb_trn_err = repcep(gb_trans[0], gb_trans[1], gb_trans[2], nval3)\n",
    "        trel_pd_gt = reprep(c_all_pd_t_rel, c_all_gt_t_rel)\n",
    "        trel_err = repcep(trel_pd_gt[0], trel_pd_gt[1], trel_pd_gt[2], nval4)\n",
    "        \n",
    "        ms = 3\n",
    "        clear_output(wait=True)  \n",
    "        dirs = ['X', 'Y', 'Z']\n",
    "        plt.figure(figsize=(18, 5))\n",
    "        for i in range(3):\n",
    "                plt.subplot(1, 3, i+1)\n",
    "                plt.plot(rot_rell[i][-1], '--*', markersize=ms, label='GT_rel_rot_'+dirs[i]+' deg')\n",
    "                plt.plot(rot_rell[i][0], '--*', markersize=ms, label='pred_rel_rot_'+dirs[i]+\n",
    "                        '\\nCEP: '+str(round(rot_err_rell[i][0],3))+' deg.')\n",
    "                plt.legend()\n",
    "        plt.suptitle('Relative Rotation Estimates')\n",
    "        plt.savefig('Relative_Rotation_Estimatesc_128_nicp_nsup4_sb_vit.png')\n",
    "        \n",
    "        plt.figure(figsize=(18, 5))\n",
    "        for i in range(3):\n",
    "                plt.subplot(1, 3, i+1)\n",
    "                plt.plot(trel_pd_gt[i][-1], '--*', markersize=ms, label='GT_rel_trans_'+dirs[i]+' m')\n",
    "                plt.plot(trel_pd_gt[i][0], '--*', markersize=ms, label='calc_rel_trans_'+dirs[i]+\n",
    "                        '\\nCEP: '+str(round(trel_err[i][0],3))+' m')\n",
    "                plt.legend()\n",
    "        plt.suptitle('Computed Relative Translation Estimates')\n",
    "        \n",
    "        plt.figure(figsize=(18, 5))\n",
    "        for i in range(3):\n",
    "                plt.subplot(1, 3, i+1)\n",
    "                plt.plot(gb_rot[i][-1], '--*', markersize=ms, label='GT_rot_'+dirs[i]+' deg')\n",
    "                plt.plot(gb_rot[i][0], '--*', markersize=ms, label='pred_rot_'+dirs[i]+\n",
    "                        '\\nCEP: '+str(round(gb_rot_err[i][0],3))+' deg.')\n",
    "                plt.legend()\n",
    "        plt.suptitle('Global Rotation Estimates')\n",
    "\n",
    "        plt.figure(figsize=(18, 5))\n",
    "        for i in range(3):\n",
    "                plt.subplot(1, 3, i+1)\n",
    "                plt.plot(gb_trans[i][-1], '--*', markersize=ms, label='GT_trans_'+dirs[i]+' m')\n",
    "                plt.plot(gb_trans[i][0], '--*', markersize=ms, label='pred_trans_'+dirs[i]+\n",
    "                        '\\nCEP: '+str(round(gb_trn_err[i][0],3))+' m')\n",
    "                plt.legend()\n",
    "        plt.suptitle('Global Translation Estimates')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_rots(qv, thr=25, nu=True):\n",
    "    \"\"\"qv is Nx4 numpy array of quartenion in qw, qx, qy, qz\"\"\"\n",
    "    ind1 = np.arange(len(qv)) # \n",
    "    qx, qy, qz, qw = qv[:,1].reshape(-1,1), qv[:,2].reshape(-1,1), qv[:,3].reshape(-1,1), qv[:,0].reshape(-1,1)\n",
    "    pred_q_ck_ci = np.hstack((qx, qy, qz, qw))\n",
    "    l2_norm = np.linalg.norm(pred_q_ck_ci, axis=1, keepdims=True)\n",
    "    pred_q_ck_ci = pred_q_ck_ci/l2_norm\n",
    "    pred_eul_ck_ci = R.from_quat(pred_q_ck_ci).as_euler('xyz', degrees=True) #Nx3 euler angles\n",
    "    pred_R_ck_ci = R.from_quat(pred_q_ck_ci).as_matrix()\n",
    "    pred_R_ci_ck = pred_R_ck_ci.transpose(0,2,1)\n",
    "    p_ind = np.all(np.abs(pred_eul_ck_ci) <= thr, axis=1).tolist()\n",
    "    \n",
    "    if nu:\n",
    "        pred_eul_ck_ci = pred_eul_ck_ci[p_ind, :]\n",
    "        pred_R_ci_ck = pred_R_ci_ck[p_ind, :, :]\n",
    "    return p_ind, pred_R_ci_ck, pred_eul_ck_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_trans(tv, thr=2.5):\n",
    "    p_ind = np.all(np.abs(tv)<= thr, axis=1).tolist()\n",
    "    return p_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_to_matrix2(qv):\n",
    "    qx, qy, qz, qw = qv[:,1].reshape(-1,1), qv[:,2].reshape(-1,1), qv[:,3].reshape(-1,1), qv[:,0].reshape(-1,1)\n",
    "    pred_q_ck_ci = np.hstack((qx, qy, qz, qw))\n",
    "    l2_norm = np.linalg.norm(pred_q_ck_ci, axis=1, keepdims=True)\n",
    "    pred_q_ck_ci = pred_q_ck_ci/l2_norm\n",
    "    pred_R_ck_ci = R.from_quat(pred_q_ck_ci).as_matrix()\n",
    "    pred_R_ci_ck = pred_R_ck_ci.transpose(0,2,1)\n",
    "    return pred_R_ci_ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_to_rotation_matrix2(qv):\n",
    "    qx, qy, qz = qv[:,0].reshape(-1,1), qv[:,1].reshape(-1,1), qv[:,2].reshape(-1,1)\n",
    "    pred_q_ck_ci = np.hstack((qx, qy, qz))\n",
    "    pred_R_ck_ci = R.from_euler('xyz', pred_q_ck_ci, degrees=True).as_matrix()\n",
    "    pred_R_ci_ck = pred_R_ck_ci.transpose(0,2,1)\n",
    "    return pred_R_ci_ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_eval2(rgbd_tsr, p_gt, mod1):\n",
    "    modn = mod1.eval()\n",
    "    with torch.no_grad():\n",
    "        gt_T1_tsr, gt_Q1_tsr, gt_T2_tsr, gt_Q2_tsr, gt_T_rel_k_i_tsr, gt_Q_rel_ck_ci_tsr, flags = p_gt\n",
    "        rgbd1_tsr_src, rgbd2_tsr_temp = rgbd_tsr\n",
    "        f1_rgb = modn.model(rgbd1_tsr_src) \n",
    "        f2_rgb = modn.model(rgbd2_tsr_temp) \n",
    "        \n",
    "        B1, D1 = f1_rgb.shape\n",
    "        B2, D2 = f2_rgb.shape\n",
    "        f1_rgb = f1_rgb.unsqueeze(1)  # Shape: B1 x 1 x D1\n",
    "        f2_rgb = f2_rgb.unsqueeze(0)  # Shape: 1 x B2 x D2\n",
    "        out_prod = f1_rgb.expand(B1, B2, D1) - f2_rgb.expand(B1, B2, D2)\n",
    "        p_wxyz = modn.lin4c(out_prod)\n",
    "        p_xyz = modn.lin4d(out_prod)\n",
    "        pred_Q_rel_ck_ci_tsr = p_wxyz\n",
    "        pred_T_rel_ck_ci_tsr = p_xyz\n",
    "        feas_temps = f2_rgb.squeeze(0).cpu().numpy()\n",
    "        all_plt = []\n",
    "        N1 = gt_Q1_tsr.shape[0]\n",
    "\n",
    "        all_gt_trans, all_gt_rot = [], []\n",
    "        all_pd_trans, all_pd_rot = [], []\n",
    "        all_gt_t_rel, all_pd_t_rep_ind1l = [], []\n",
    "        all_gt_rot_rel, all_pd_rot_rel = [], []\n",
    "        all_pd_t_rel = []\n",
    "        for i in range(N1):  \n",
    "            pred_qi_ck_ci = pred_Q_rel_ck_ci_tsr[i, :, 0:4].cpu().numpy()\n",
    "            p_ind1, pred_Ri_ci_ck, pred_eul_ck_ci = valid_rots(pred_qi_ck_ci, nu=False)\n",
    "            if np.any(p_ind1):\n",
    "                pred_Ri_ci_ck, pred_eul_ck_ci = pred_Ri_ci_ck[p_ind1, :, :], pred_eul_ck_ci[p_ind1, :]\n",
    "                _, _, gt_rel_rot_i = valid_rots(gt_Q_rel_ck_ci_tsr[i, :, 0:4].cpu().numpy(), nu=False)\n",
    "                gt_rel_rot_i = gt_rel_rot_i[p_ind1, :]\n",
    "                feas_i = f1_rgb.squeeze(1)[i, :].cpu().numpy().reshape(1,-1)\n",
    "                gt_R_g_ci = quat_to_R_matrix(gt_Q1_tsr[i,:]).cpu().numpy()\n",
    "                gt_rpy_g_ci = clip_val(np.array(R.from_matrix(gt_R_g_ci).as_euler('xyz', degrees=True)))\n",
    "\n",
    "                gt_Q2_tsr_i = gt_Q2_tsr[p_ind1, :].cpu().numpy()\n",
    "                gt_R_g_ck = quaternion_to_matrix2(gt_Q2_tsr_i).transpose(0,2,1)\n",
    "                pred_R_g_ci = np.matmul(pred_Ri_ci_ck.transpose(0,2,1), gt_R_g_ck)\n",
    "                pred_rpy_g_ci = clip_val(np.array(R.from_matrix(pred_R_g_ci).as_euler('xyz', degrees=True)))\n",
    "\n",
    "                if np.any(p_ind1):\n",
    "                    feas_2 = feas_temps[p_ind1,:]\n",
    "                    cs_sim = np.dot(feas_2, feas_i.T) / (np.linalg.norm(feas_i) * np.linalg.norm(feas_2, axis=1, keepdims=True))\n",
    "                    wt_i = cs_sim/cs_sim.sum()\n",
    "                    wt_i = wt_i.reshape(-1,1)\n",
    "                    if len(pred_rpy_g_ci)>=1:\n",
    "                        pred_rpy_bi_g_val = np.sum(wt_i*pred_rpy_g_ci, 0).flatten()\n",
    "                        all_pd_rot.append(pred_rpy_bi_g_val)\n",
    "                        all_gt_rot.append(gt_rpy_g_ci.flatten())\n",
    "                    all_gt_rot_rel.append(gt_rel_rot_i)\n",
    "                    all_pd_rot_rel.append(pred_eul_ck_ci)\n",
    "        \n",
    "        N2 = gt_T1_tsr.shape[0]\n",
    "        for i in range(N2):  \n",
    "            pred_T_rel_ck_ci_tsr = pred_T_rel_ck_ci_tsr[0:N2, :, :]\n",
    "            pred_ti_ck_ci = pred_T_rel_ck_ci_tsr[i, :, 0:3].cpu().numpy()\n",
    "            p_ind1 = valid_trans(pred_ti_ck_ci)\n",
    "            if np.any(p_ind1):\n",
    "                feas_i = f1_rgb.squeeze(1)[i, :].cpu().numpy().reshape(1,-1)\n",
    "                gt_T_g_ci = gt_T1_tsr[i, :].flatten().cpu().numpy()\n",
    "                pred_ti_ck_ci = pred_ti_ck_ci[p_ind1, :]\n",
    "                gt_Q2_tsr_i = gt_Q2_tsr[p_ind1, :].cpu().numpy()\n",
    "                gt_R_g_ck = quaternion_to_matrix2(gt_Q2_tsr_i).transpose(0,2,1)\n",
    "                gt_T2_tsr_i = gt_T2_tsr[p_ind1, :]\n",
    "                T_g_ck = gt_T2_tsr_i.cpu().numpy()\n",
    "                pred_T_ck_ci = pred_ti_ck_ci[:,:,np.newaxis]\n",
    "                gt_R_ck_g = gt_R_g_ck.transpose(0,2,1)\n",
    "                pred_T_g_ci = T_g_ck + (np.matmul(gt_R_ck_g, pred_T_ck_ci)).squeeze(2)\n",
    "                if np.any(p_ind1):\n",
    "                    feas_2 = feas_temps[p_ind1,:]\n",
    "                    cs_sim = np.dot(feas_2, feas_i.T) / (np.linalg.norm(feas_i) * np.linalg.norm(feas_2, axis=1, keepdims=True))\n",
    "                    wt_i = cs_sim/cs_sim.sum()\n",
    "                    wt_i = wt_i.reshape(-1,1)\n",
    "                    p_pred_T_g_ci = pred_T_g_ci\n",
    "                    if len(p_pred_T_g_ci)>=1:\n",
    "                        pred_T_g_ci_val = np.sum(wt_i*p_pred_T_g_ci, 0).flatten()\n",
    "                        all_pd_trans.append(pred_T_g_ci_val)\n",
    "                        all_gt_trans.append(gt_T_g_ci)\n",
    "                    gt_trel = gt_T_rel_k_i_tsr[i, :, 0:3][p_ind1, :].cpu().numpy()\n",
    "                    all_gt_t_rel.append(gt_trel)\n",
    "                    all_pd_t_rel.append(pred_ti_ck_ci)\n",
    "        rots = (np.array(all_pd_rot), np.array(all_gt_rot))\n",
    "        trans = (np.array(all_pd_trans), np.array(all_gt_trans)) \n",
    "        if len(all_gt_rot_rel)>=1 and len(all_gt_t_rel)>=1:\n",
    "            all_gt_t_rel = np.vstack(all_gt_t_rel) \n",
    "            all_pd_t_rel = np.vstack(all_pd_t_rel) \n",
    "            all_gt_rot_rel = np.vstack(all_gt_rot_rel)  \n",
    "            all_pd_rot_rel = np.vstack(all_pd_rot_rel)\n",
    "            rel_rots = (all_pd_rot_rel, all_gt_rot_rel)   \n",
    "            return rots, trans, all_gt_t_rel, all_pd_t_rel, rel_rots, all_plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "## load the source images, their translations wrt G and quaternions denoting rotations from G to the camera frame\n",
    "## If Gazebo is used these images and their poses can be subcribed to and obtained using ROS in realtime during training\n",
    "## If the background contains diverse elements, it may be necessary to mask them using a pretrained model, as suggested in the paper.\n",
    "\n",
    "path_to_src_imgs = './source_images/'\n",
    "path_to_src_translations = './source_images/source_trans.npy'#'./...'\n",
    "path_to_src_quaternions = './source_images/source_quat.npy'#'./...'\n",
    "T1_list = np.load(path_to_src_translations).tolist()\n",
    "Q1_list = np.load(path_to_src_quaternions).tolist()\n",
    "img1_list = []\n",
    "for i in range(len(T1_list)):\n",
    "    img1_list.append(cv2.imread(path_to_src_imgs+'img_'+str(i)+'.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "## load the template images, their translations wrt G and quaternions denoting rotations from G to the camera frame\n",
    "## If Gazebo is used, it's recommended to first capture and store the template images and poses, then load them during training and deployment..\n",
    "path_to_temp_imgs = './template_images/' #'./...'\n",
    "path_to_temp_translations = './template_images/temp_poseTrans_.npy'#'./...'\n",
    "path_to_temp_quaternions = './template_images/temp_poseQuat_.npy'#'./...'\n",
    "T2_list = np.load(path_to_temp_translations).tolist()\n",
    "Q2_list = np.load(path_to_temp_quaternions).tolist()\n",
    "img2_list = []\n",
    "for i in range(len(T2_list)):\n",
    "    img2_list.append(cv2.imread(path_to_temp_imgs+'temp_'+str(i)+'.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_loss = 1000000\n",
    "comb_loss = []\n",
    "all_global_ests = []\n",
    "all_calc_r, all_pred_r, all_gt_r = [], [], []\n",
    "all_calc_t, all_pred_t, all_gt_t = [], [], []\n",
    "all_rel_rot_pd, all_rel_rot_gt, all_rel_tr_pd, all_rel_tr_gt = [], [], [], []\n",
    "all_global_rot_pd, all_global_rot_gt, all_global_t_pd, all_global_t_gt = [], [], [], []\n",
    "a_nval2 = 0\n",
    "a_nval3 = 0\n",
    "a_nval4 = 0\n",
    "c_all_gt_t_rel, c_all_pd_t_rel = [], []\n",
    "b_size_src = 20 ## set according to your GPU capacity\n",
    "b_size_temp = 20 ## set according to your GPU capacity\n",
    "nepochs = 500\n",
    "for epoch in range(nepochs):\n",
    "    b_inds_src = random.sample(range(len(T1_list)), b_size_src)\n",
    "    b_inds_temp = random.sample(range(len(T2_list)), b_size_temp)\n",
    "    T1_list_bs = [T1_list[i] for i in b_inds_src]\n",
    "    Q1_list_bs = [Q1_list[i] for i in b_inds_src]\n",
    "    img1_list_bs = [img1_list[i] for i in b_inds_src]\n",
    "\n",
    "    T2_list_bs = [T2_list[i] for i in b_inds_temp]\n",
    "    Q2_list_bs = [Q2_list[i] for i in b_inds_temp]\n",
    "    img2_list_bs = [img2_list[i] for i in b_inds_temp]\n",
    "    \n",
    "    out1 = cal_gt_relpose_360(T1_list_bs, Q1_list_bs, T2_list_bs, Q2_list_bs)\n",
    "    T_rel_tsr, Q_rel_tsr, flags_tsr = out1\n",
    "    gflag = geom_cstr_comb_flags360(T1_list_bs, Q1_list_bs, T2_list_bs, Q2_list_bs)\n",
    "    img_src_tsr, img_temp_tsr = prep_inputs3(img1_list_bs, img2_list_bs)\n",
    "    T1_tsr, Q1_tsr = torch.tensor(T1_list_bs).float().to(device), torch.tensor(Q1_list_bs).float().to(device)\n",
    "    T2_tsr, Q2_tsr = torch.tensor(T2_list_bs).float().to(device), torch.tensor(Q2_list_bs).float().to(device)\n",
    "    out2 = training_loop(mod1, optimz, img_src_tsr, img_temp_tsr, T_rel_tsr, Q_rel_tsr, \n",
    "                         flags_tsr, gflag, T1_tsr, Q1_tsr, T2_tsr, Q2_tsr, criterion1)\n",
    "\n",
    "    if out2 is not None:\n",
    "        comb_loss.append(out2)\n",
    "        if out2[-1] < prev_loss:\n",
    "            torch.save(mod1.state_dict(), './gbdtposenet.pth')\n",
    "            prev_loss = out2[-1]\n",
    "        \n",
    "    if epoch%10==0 and len(comb_loss)>=1:\n",
    "        plot_progress(comb_loss)\n",
    "\n",
    "    run_eval = True\n",
    "    if epoch%15==0 and run_eval:\n",
    "        b_inds_src = random.sample(range(len(T1_list)), b_size_src)\n",
    "        b_inds_temp = random.sample(range(len(T1_list)), b_size_src)\n",
    "        T1_list_bs_test = [T1_list[i] for i in b_inds_src]\n",
    "        Q1_list_bs_test = [Q1_list[i] for i in b_inds_src]\n",
    "        img1_list_bs_test = [img1_list[i] for i in b_inds_src]\n",
    "        \n",
    "        T2_list_bs_test = [T2_list[i] for i in b_inds_temp]\n",
    "        Q2_list_bs_test = [Q2_list[i] for i in b_inds_temp]\n",
    "        img2_list_bs_test = [img2_list[i] for i in b_inds_temp]\n",
    "\n",
    "\n",
    "        out3 = cal_gt_relpose_360(T1_list_bs_test, Q1_list_bs_test, T2_list_bs_test, Q2_list_bs_test)\n",
    "        T_rel_tsr_test, Q_rel_tsr_test, flags_tsr_test = out3\n",
    "        rgbd_tsr_test = prep_inputs3(img1_list_bs_test, img2_list_bs_test)\n",
    "        T1_tsr_test, Q1_tsr_test = torch.tensor(T1_list_bs_test).float().to(device), torch.tensor(Q1_list_bs_test).float().to(device)\n",
    "        T2_tsr_test, Q2_tsr_test = torch.tensor(T2_list_bs_test).float().to(device), torch.tensor(Q2_list_bs_test).float().to(device)\n",
    "\n",
    "        p_gt = (T1_tsr_test, Q1_tsr_test, T2_tsr_test, Q2_tsr_test, T_rel_tsr_test, Q_rel_tsr_test, flags_tsr_test)\n",
    "        out4 = global_eval2(rgbd_tsr_test, p_gt, mod1)\n",
    "\n",
    "        if out4 is not None:\n",
    "            rots_gb, trans_gb, all_gt_t_rel, all_pd_t_rel, rots_m, all_plt = out4\n",
    "            all_rel_rot_pd.append(rots_m[0]), all_rel_rot_gt.append(rots_m[-1])\n",
    "            all_global_rot_pd.append(rots_gb[0]), all_global_rot_gt.append(rots_gb[-1])\n",
    "            all_global_t_pd.append(trans_gb[0]), all_global_t_gt.append(trans_gb[-1])\n",
    "            c_all_gt_t_rel.append(all_gt_t_rel), c_all_pd_t_rel.append(all_pd_t_rel)\n",
    "        \n",
    "            nval2 = rots_m[0].shape[0]\n",
    "            nval3 = rots_gb[0].shape[0]\n",
    "            nval4 = all_gt_t_rel.shape[0]\n",
    "            a_nval2 += nval2\n",
    "            a_nval3 += nval3\n",
    "            a_nval4 += nval4\n",
    "            plot_global_est2(all_calc_r, all_pred_r, all_gt_r, all_calc_t, all_pred_t, all_gt_t, \n",
    "                                all_rel_rot_pd, all_rel_rot_gt, all_rel_tr_pd, all_rel_tr_gt, a_nval2, \n",
    "                                all_global_rot_pd, all_global_rot_gt, all_global_t_pd, all_global_t_gt, \n",
    "                                a_nval3, a_nval4, c_all_gt_t_rel, c_all_pd_t_rel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
